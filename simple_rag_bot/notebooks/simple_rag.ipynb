{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW9MwnF56b-0"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yPUUFMUKhdn",
        "outputId": "0ac0c2c8-ffa0-4d2e-90e6-5df38ea0af99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q pymupdf==1.25.3 \\\n",
        "                ratelimit==2.2.1\\\n",
        "                langchain-text-splitters==0.3.6 \\\n",
        "                langchain-core==0.3.41\\\n",
        "                langchain-google-genai==2.0.11 \\\n",
        "                pymilvus==2.5.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsC0SbfTezgE"
      },
      "source": [
        "#### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBTmiDr9LEcX",
        "outputId": "3cba191c-8751-4431-b84b-4036e31ff0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ognLVOY5hNv6"
      },
      "source": [
        "#### Initialize required variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5MHIuHwGku7"
      },
      "outputs": [],
      "source": [
        "GEMINI_API_KEY = \"API-KEY\" #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mf7yZrPLLpf"
      },
      "outputs": [],
      "source": [
        "# Path to folder containing rag pdfs\n",
        "rag_files_path = \"/content/drive/MyDrive/GenAI_Expts/rag_datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cnpqRwL9860"
      },
      "outputs": [],
      "source": [
        "# Path to embeddings jsonl file (if created already), instead of chunking and creating embeddings all over again\n",
        "embeddings_path = \"/content/drive/MyDrive/GenAI_Expts/cleaned_embeddings.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sI6D1D-e14m"
      },
      "source": [
        "#### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jhzpyj-LvJS"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import fitz\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "from tqdm import tqdm\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbs4RM83QPnA"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "from pymilvus import MilvusClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8C0jrH5fJPA"
      },
      "source": [
        "#### Util functions to load content from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApADlmbtmU5v"
      },
      "outputs": [],
      "source": [
        "def get_files_in_dir(path):\n",
        "  \"\"\"Get complete path of the pdf files in dir\"\"\"\n",
        "  files_list = []\n",
        "\n",
        "  for file in listdir(path):\n",
        "    file_path = join(path, file)\n",
        "    if file_path.endswith(\".pdf\") and isfile(file_path):\n",
        "      files_list.append(file_path)\n",
        "\n",
        "  return files_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha8vFvn5-Qbr"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(path):\n",
        "  \"\"\"Load embeddings from jsonl file\"\"\"\n",
        "  with open(path, 'r') as json_file:\n",
        "    json_data = [json.loads(line) for line in json_file]\n",
        "\n",
        "  return json_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC4p_C7jPkpU"
      },
      "source": [
        "### Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhfQo5zXyvE5"
      },
      "source": [
        "Simple recursive text chunking that splits text into chunks of size 1000 and an overlap of 200 tokens.\n",
        "- The chunking done here, is the most simple one and works with pdf text, i.e. pdf containing machine readable text or digital text.\n",
        "- For pdf containing images or scanned content, another layer like OCR or custom parser should be used. Same applies if other types of files like ppt, docx etc are used.\n",
        "- Other chunking strategies like context aware chunking, Semantic chunking etc.\n",
        "\n",
        "To keep things simple, the most basic of basic chunking strategies were used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEw2gD0hcvxm"
      },
      "outputs": [],
      "source": [
        "class PDFTextSplitter:\n",
        "  \"\"\"Document splitter class for pdf text\"\"\"\n",
        "\n",
        "  def __init__(self, file_uri, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Initialize text splitter with required params\"\"\"\n",
        "    self.file_uri = file_uri\n",
        "    self.chunk_size = chunk_size\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "    self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=self.chunk_size,\n",
        "        chunk_overlap=self.chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "  def _clean_special_chars(self, text: str):\n",
        "    \"\"\"Remove special characters/unicodes in the text\"\"\"\n",
        "    cleaned_text = text.replace(\"\\t\\n\",\"\").replace(\"\\x08\\n\",\"\").replace(\"\\xa0\",\"\").replace(\"\\t\\r\",\"\").replace(\"\\uf0b7\",\"\")\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "  def get_page_info(self, pdf_path: str):\n",
        "    \"\"\"Get full text content of pdf and page info\"\"\"\n",
        "    doc_id = str(uuid.uuid4())\n",
        "    full_text = \"\"\n",
        "    page_info = []\n",
        "\n",
        "    pdf_doc = fitz.open(pdf_path)\n",
        "\n",
        "    for page_num in range(pdf_doc.page_count):\n",
        "      page = pdf_doc[page_num]\n",
        "      text = page.get_text()\n",
        "      page_start_idx = len(full_text)\n",
        "      full_text += text\n",
        "      page_end_idx = len(full_text)\n",
        "      page_info.append({\n",
        "          \"doc_id\": doc_id,\n",
        "          \"page_id\": page_num,\n",
        "          \"page_number\": page_num + 1,\n",
        "          \"start_char_idx\": page_start_idx,\n",
        "          \"end_char_idx\": page_end_idx,\n",
        "      })\n",
        "    full_text = self._clean_special_chars(full_text)\n",
        "\n",
        "    return doc_id, full_text, page_info\n",
        "\n",
        "  def get_chunks_with_info(self, pdf_uri: str, pdf_text: str, page_info: list, doc_id: str):\n",
        "    \"\"\"Split pdf content to chunks and return metadata dict\"\"\"\n",
        "    chunks = []\n",
        "    text_chunks = self.text_splitter.split_text(pdf_text)\n",
        "\n",
        "    start_idx = 0\n",
        "    chunk_positions = []\n",
        "\n",
        "    # Find the chunk start and end index in the pdf text\n",
        "    for chunk in tqdm(text_chunks):\n",
        "      chunk_start = pdf_text.find(chunk, start_idx)\n",
        "      if chunk_start == -1:\n",
        "        continue\n",
        "\n",
        "      chunk_end = chunk_start + len(chunk)\n",
        "      chunk_positions.append((chunk_start, chunk_end))\n",
        "      start_idx = chunk_start + 1\n",
        "\n",
        "    # Identify which pages the chunk falls under, lowest and highest number of the page will be the page span\n",
        "    for i, (chunk, (start_pos, end_pos)) in enumerate(zip(text_chunks, chunk_positions)):\n",
        "      spanning_pages = []\n",
        "      for page in page_info:\n",
        "        if (start_pos < page[\"end_char_idx\"] and end_pos > page[\"start_char_idx\"]):\n",
        "          spanning_pages.append(page[\"page_number\"])\n",
        "\n",
        "      if not spanning_pages:\n",
        "        continue\n",
        "\n",
        "      chunk_data = {\n",
        "          \"doc_id\": doc_id,\n",
        "          \"chunk_id\": f\"{doc_id}/chunks/c{i}\",\n",
        "          \"content\": chunk,\n",
        "          \"page_span\": spanning_pages,\n",
        "          \"chunk_metadata\": {\n",
        "              \"start_char_idx\": start_pos,\n",
        "              \"end_char_idx\": end_pos\n",
        "          },\n",
        "          \"document_metadata\": {\n",
        "              \"url\": pdf_uri,\n",
        "              \"title\": pdf_uri.split(\"/\")[-1]\n",
        "          }\n",
        "      }\n",
        "\n",
        "      chunks.append(chunk_data)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "  def process_documents(self, file_uri: list=[]):\n",
        "    \"\"\"Process documents to chunks\"\"\"\n",
        "    if not file_uri:\n",
        "      file_uri = self.file_uri\n",
        "\n",
        "    if not isinstance(file_uri, list):\n",
        "      file_uri = [file_uri]\n",
        "\n",
        "    chunked_file_data = []\n",
        "    for pdf_path in file_uri:\n",
        "      print(f\"Processing {pdf_path}\")\n",
        "      doc_id, pdf_text, page_info = self.get_page_info(pdf_path)\n",
        "      chunks_data = self.get_chunks_with_info(pdf_path, pdf_text, page_info, doc_id)\n",
        "      chunked_file_data.extend(chunks_data)\n",
        "\n",
        "    print(f\"Chunking completed for {pdf_path}.\")\n",
        "    return chunked_file_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VepEsWRofjlW"
      },
      "outputs": [],
      "source": [
        "document_splitter = PDFTextSplitter(rag_files_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twddGb0wl16L"
      },
      "outputs": [],
      "source": [
        "all_files_list = get_files_in_dir(rag_files_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUjwG2e2k1co",
        "outputId": "02e63653-16d8-4964-9856-427e7644decb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/Newwhitepaper_Agents2.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 106589.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 41995.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/Introduction_to_climate_change_FINAL_002.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 34211.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/understanding-climate-change-2008.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 51247.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/Understanding_Climate_Change.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 92/92 [00:00<00:00, 55426.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/LLM Powered Autonomous Agents _ Lil'Log.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [00:00<00:00, 100932.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/GenAI_Expts/rag_datasets/20 Easy International Recipes.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 82241.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunking completed for /content/drive/MyDrive/GenAI_Expts/rag_datasets/20 Easy International Recipes.pdf.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "all_docs_chunked_data = document_splitter.process_documents(all_files_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YBGeagmLj6n0",
        "outputId": "c4f25daa-143d-4a4c-eaba-979086fbe637"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'doc_id': 'e436348e-a04d-4a66-b325-e7aca5259704',\n",
              "  'chunk_id': 'e436348e-a04d-4a66-b325-e7aca5259704/chunks/c0',\n",
              "  'content': 'Agents\\nAuthors: Julia Wiesinger, Patrick Marlow \\nand Vladimir Vuskovic\\nAgents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning \\nIntroduction4\\nWhat is an agent?5\\nThe model6\\nThe tools7\\nThe orchestration layer7\\nAgents vs. models8\\nCognitive architectures: How agents operate 8\\nTools: Our keys to the outside world12\\nExtensions 13\\nSample Extensions 15\\nFunctions 18\\nUse cases21\\nFunction sample code24\\nData stores27\\nImplementation and application28\\nTools recap32\\nEnhancing model performance with targeted learning33\\nAgent quick start with LangChain35\\nProduction applications with Vertex AI agents38\\nSummary40\\nEndnotes42\\nTable of contents\\nAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools',\n",
              "  'page_span': [1, 2, 3, 4],\n",
              "  'chunk_metadata': {'start_char_idx': 0, 'end_char_idx': 996},\n",
              "  'document_metadata': {'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Newwhitepaper_Agents2.pdf',\n",
              "   'title': 'Newwhitepaper_Agents2.pdf'}},\n",
              " {'doc_id': 'e436348e-a04d-4a66-b325-e7aca5259704',\n",
              "  'chunk_id': 'e436348e-a04d-4a66-b325-e7aca5259704/chunks/c1',\n",
              "  'content': \"Summary40\\nEndnotes42\\nTable of contents\\nAgents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion. Just like humans, Generative AI models can be trained to use tools \\nto access real-time information or suggest a real-world action. For example, a model can \\nleverage a database retrieval tool to access specific information, like a customer's purchase \\nhistory, so it can generate tailored shopping recommendations. Alternatively, based on a \\nuser's query, a model can make various API calls to send an email response to a colleague \\nor complete a financial transaction on your behalf. To do so, the model must not only have \\naccess to a set of external tools, it needs the ability to plan and execute any task in a self-\\ndirected fashion. This combination of reasoning, logic, and access to external information\",\n",
              "  'page_span': [3, 4],\n",
              "  'chunk_metadata': {'start_char_idx': 830, 'end_char_idx': 1827},\n",
              "  'document_metadata': {'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Newwhitepaper_Agents2.pdf',\n",
              "   'title': 'Newwhitepaper_Agents2.pdf'}}]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Seeing the chunk content\n",
        "all_docs_chunked_data[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hSIWMP2Pm3n"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk1OzYWrCEZy"
      },
      "source": [
        "Local models\n",
        "- all-MiniLM-L6-v2 - faster and mostly accurate\n",
        "- all-mpnet-base-v2 - slightly better accuracy but slower\n",
        "\n",
        "API\n",
        "- Google, OpenAI, Above models with HF Token.\n",
        "\n",
        "Local models are difficult to run with current specs of my system, since the goal was to run this as an application. So only option is to use APIs, Google's embedding models as they are free and superior and get's the job done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGwt_n-ynaPv"
      },
      "outputs": [],
      "source": [
        "class EmbeddingClient:\n",
        "  \"\"\"Custom class for embedding data with Gemini models\"\"\"\n",
        "  def __init__(self, model_name: str=\"models/text-embedding-004\", embedding_api_key: str=\"\"):\n",
        "    \"\"\"Initialize embedding client with required params\"\"\"\n",
        "    self.model_name = model_name\n",
        "    self.embedding_api_key = embedding_api_key\n",
        "    self.embedding_instance = GoogleGenerativeAIEmbeddings(model=self.model_name,\n",
        "                                                           google_api_key=self.embedding_api_key)\n",
        "\n",
        "  @sleep_and_retry\n",
        "  @limits(calls=120, period=60)\n",
        "  def _generate_embedding(self, chunk_content):\n",
        "    \"\"\"Helper function to generate embedding with rate limiting\"\"\"\n",
        "    try:\n",
        "      return self.embedding_instance.embed_query(chunk_content)\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "  def generate_embeddings(self, chunks_data):\n",
        "    \"\"\"Generate embeddings for content\"\"\"\n",
        "\n",
        "    embeddings_data = copy.deepcopy(chunks_data)\n",
        "\n",
        "    for chunk in tqdm(embeddings_data):\n",
        "      chunk_content = chunk[\"content\"]\n",
        "      chunk_embedding = chunk_embedding = self._generate_embedding(chunk_content)\n",
        "      chunk[\"chunk_embedding\"] = chunk_embedding\n",
        "\n",
        "    return embeddings_data\n",
        "\n",
        "  def get_query_embeddings(self, content: str):\n",
        "    \"\"\"Generate embeddings for query during runtime\"\"\"\n",
        "\n",
        "    query_embedding = self.embedding_instance.embed_query(text=content)\n",
        "\n",
        "    return query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74b9ZUzeoykE"
      },
      "outputs": [],
      "source": [
        "embedding_instance = EmbeddingClient(embedding_api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjEdQ7w8pj6C",
        "outputId": "b70ffb81-e885-43e5-a695-fbcd1871c88a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 332/332 [02:11<00:00,  2.53it/s]\n"
          ]
        }
      ],
      "source": [
        "embeddings_data = embedding_instance.generate_embeddings(all_docs_chunked_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpSnuaeHytl8"
      },
      "outputs": [],
      "source": [
        "with open('cleaned_embeddings.jsonl', 'w') as outfile:\n",
        "  for entry in embeddings_data:\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a422iax-Pqx4"
      },
      "source": [
        "### Indexing data to VectorDB\n",
        "Embeddings can be loaded from the above process or directly from the drive.\n",
        "\n",
        "For experimentation, once the embeddings are created they are stored as jsonl file and saved in drive. The file can be loaded from next time to avoid chunking and embedding again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W9i1Ln_-fJg"
      },
      "outputs": [],
      "source": [
        "embedding_data = load_embeddings(embeddings_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf8ZaU0a_7Ly",
        "outputId": "f74e5557-b6dc-4d49-c8e7-5b49b5ce9c90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['doc_id', 'chunk_id', 'content', 'page_span', 'chunk_metadata', 'document_metadata', 'chunk_embedding'])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_data[0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ve-FbxwwoqX"
      },
      "source": [
        "Milvus is a vector database know for it's speed and features. Some of the reasons for choosing Milvus over other vectore databases like ChromaDB,Qdrant etc are,\n",
        "- Getting familiar with production grade vector db. Although chromdb is simple and light, milvus is used in a lot of production projects. Mostly for it's scalability and features.\n",
        "- Metadata filtering and parsing - Milvus has better features when it comes to this.\n",
        "- Typical curiosity to use milvus, since i've experimented with chromadb and other vector db before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xM_-U4CGFlh"
      },
      "outputs": [],
      "source": [
        "class CustomMilvusClient:\n",
        "  \"\"\"Custom class for Milvus Client to create, update and query a milvus collection\"\"\"\n",
        "  def __init__(self, uri: str):\n",
        "    self.uri = uri\n",
        "    self.milvus_client = MilvusClient(uri=self.uri)\n",
        "\n",
        "  def create_collection(self, collection_name: str=\"\", embedding_dimension: int=0, vector_field_name: str=\"\", primary_field_name: str=\"\", max_id_length: int=50):\n",
        "    \"\"\"Create collection with given fields into milvus vector db\"\"\"\n",
        "    if self.milvus_client.has_collection(collection_name):\n",
        "      print(f\"Collection with {collection_name} already exists. Over writing existing collection.\")\n",
        "      self.milvus_client.drop_collection(collection_name)\n",
        "\n",
        "    try:\n",
        "      self.milvus_client.create_collection(collection_name=collection_name,\n",
        "                                          dimension=embedding_dimension,\n",
        "                                          vector_field_name=vector_field_name,\n",
        "                                          metric_type=\"COSINE\",\n",
        "                                          primary_field_name=primary_field_name,\n",
        "                                          id_type=\"string\",\n",
        "                                          max_length=max_id_length)\n",
        "      print(f\"Collection '{collection_name}' successfully created.\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "  def insert_data_to_collection(self, collection_name: str=\"\", data: list=[]):\n",
        "    \"\"\"Insert data into created collection in milvus db\"\"\"\n",
        "    if self.milvus_client.has_collection(collection_name):\n",
        "      self.milvus_client.insert(collection_name=collection_name,\n",
        "                         data=data)\n",
        "      print(f\"Data successfully inserted to collection '{collection_name}'.\")\n",
        "    else:\n",
        "      raise ValueError(f\"Collection with name '{collection_name}' does not exist. Please insert into another or create a new collection using .create_collection.\")\n",
        "\n",
        "  def query_collection(self, collection_name: str=\"\", query_embedding: list=[], limit: int=5, output_fields: list=[]):\n",
        "    \"\"\"Get relevant docs based on similarity between query embedding and vectors in DB\"\"\"\n",
        "    start_time = time.time()\n",
        "    if self.milvus_client.has_collection(collection_name):\n",
        "      retriever_result = self.milvus_client.search(collection_name=collection_name,\n",
        "                                            data=[query_embedding],\n",
        "                                            limit=limit,\n",
        "                                            output_fields=output_fields)\n",
        "      end_time = time.time()\n",
        "      execution_time = end_time - start_time\n",
        "      print(f\"Retrieved in {execution_time:.6f}s.\")\n",
        "      return retriever_result\n",
        "    else:\n",
        "      raise ValueError(f\"Collection with {collection_name} does not exist. Please query on another collection or create a new collection using .create_collection.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfWcjKYFriRG"
      },
      "outputs": [],
      "source": [
        "retriever_instance = CustomMilvusClient(uri=\"./my_milvus.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi7528zsIm-6",
        "outputId": "d01835da-f083-4519-d1ac-9530a7b20fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection 'pdfrag' successfully created.\n"
          ]
        }
      ],
      "source": [
        "retriever_instance.create_collection(collection_name=\"pdfrag\",\n",
        "                                     embedding_dimension=768,\n",
        "                                     vector_field_name=\"chunk_embedding\",\n",
        "                                     primary_field_name=\"chunk_id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tt0zz_XJXyU",
        "outputId": "8ac1cc42-2580-4069-bea9-27a90dcf57c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfully inserted to collection 'pdfrag'.\n"
          ]
        }
      ],
      "source": [
        "retriever_instance.insert_data_to_collection(collection_name=\"pdfrag\",\n",
        "                                             data=embedding_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcqhSmBG8GhH"
      },
      "source": [
        "### RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEyo5vzBg4Ab"
      },
      "source": [
        "#### Retriever Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsEkK44yIiKq"
      },
      "outputs": [],
      "source": [
        "# Initalize retriever and embedding clients to use at runtime\n",
        "retriever_instance = CustomMilvusClient(uri=\"./my_milvus.db\")\n",
        "embedding_instance = EmbeddingClient(embedding_api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL1zjylW5r3Y"
      },
      "outputs": [],
      "source": [
        "def get_relevant_docs(collection_name: str, query: str, top_n: int=5):\n",
        "  \"\"\"Get relevant docs for a given query\"\"\"\n",
        "  query_embedding = embedding_instance.get_query_embeddings(content=query)\n",
        "\n",
        "  docs = retriever_instance.query_collection(collection_name=\"pdfrag\",\n",
        "                                             query_embedding=query_embedding,\n",
        "                                             limit=top_n,\n",
        "                                             output_fields=[\"content\", \"page_span\",\"document_metadata\"])\n",
        "\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5c8CYHDJvab"
      },
      "outputs": [],
      "source": [
        "query = \"Type of memory in agents?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbjZ0mnTyKOD",
        "outputId": "dd5720d0-057f-48e8-ce64-c746c4d36260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.003377s.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'id': 'd07b7f7d-40e7-4aaa-999f-c9d512146bf1/chunks/c14',\n",
              "  'distance': 0.6619677543640137,\n",
              "  'entity': {'content': 'experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and\\nroutines that are performed automatically, like riding a bike or typing on a keyboard.\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\nSensory memory as learning embedding representations for raw inputs, including text,\\nimage or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the\\nfinite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time,\\naccessible via fast retrieval.\\nMaximum Inner Product Search (MIPS)\\nThe external memory can alleviate the restriction of finite attention span. A standard practice is\\nto save the embedding representation of information into a vector store database that can',\n",
              "   'page_span': [7, 8],\n",
              "   'document_metadata': {'url': \"/content/drive/MyDrive/GenAI_Expts/rag_datasets/LLM Powered Autonomous Agents _ Lil'Log.pdf\",\n",
              "    'title': \"LLM Powered Autonomous Agents _ Lil'Log.pdf\"}}},\n",
              " {'id': 'd07b7f7d-40e7-4aaa-999f-c9d512146bf1/chunks/c1',\n",
              "  'distance': 0.6501352190971375,\n",
              "  'entity': {'content': 'Reflection and refinement: The agent can do self-criticism and self-reflection over past\\nactions, learn from mistakes and refine them for future steps, thereby improving the\\nquality of final results.\\nMemory\\nShort-term memory: I would consider all the in-context learning (See Prompt\\nEngineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall\\n(infinite) information over extended periods, often by leveraging an external vector store\\nand fast retrieval.\\nTool use\\nThe agent learns to call external APIs for extra information that is missing from the model\\nweights (often hard to change after pre-training), including current information, code\\nexecution capability, access to proprietary information sources and more.\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and\\nplan ahead.',\n",
              "   'page_span': [1, 2],\n",
              "   'document_metadata': {'url': \"/content/drive/MyDrive/GenAI_Expts/rag_datasets/LLM Powered Autonomous Agents _ Lil'Log.pdf\",\n",
              "    'title': \"LLM Powered Autonomous Agents _ Lil'Log.pdf\"}}},\n",
              " {'id': 'd07b7f7d-40e7-4aaa-999f-c9d512146bf1/chunks/c12',\n",
              "  'distance': 0.6440081596374512,\n",
              "  'entity': {'content': 'Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that\\nrequire memory and exploration. Only binary reward is assigned. The source\\npolicies are trained with A3C for \"dark\" environments and DQN for\\nwatermaze.\\n(Image source: Laskin et al. 2023)\\nComponent Two: Memory\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human\\nbrain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve\\ninformation. There are several types of memory in human brains.\\n1. Sensory Memory: This is the earliest stage of memory, providing the ability to retain\\nimpressions of sensory information (visual, auditory, etc) after the original stimuli have\\nended. Sensory memory typically only lasts for up to a few seconds. Subcategories include\\niconic memory (visual), echoic memory (auditory), and haptic memory (touch).',\n",
              "   'page_span': [6, 7],\n",
              "   'document_metadata': {'url': \"/content/drive/MyDrive/GenAI_Expts/rag_datasets/LLM Powered Autonomous Agents _ Lil'Log.pdf\",\n",
              "    'title': \"LLM Powered Autonomous Agents _ Lil'Log.pdf\"}}},\n",
              " {'id': 'd07b7f7d-40e7-4aaa-999f-c9d512146bf1/chunks/c31',\n",
              "  'distance': 0.6372724175453186,\n",
              "  'entity': {'content': 'inspired by The Sims. Generative agents create believable simulacra of human behavior for\\ninteractive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection\\nmechanisms to enable agents to behave conditioned on past experience, as well as to interact\\nwith other agents.\\nMemory stream: is a long-term memory module (external database) that records a\\ncomprehensive list of agents’ experience in natural language.\\nEach element is an observation, an event directly provided by the agent. - Inter-agent\\ncommunication can trigger new natural language statements.\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to\\nrelevance, recency and importance.\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\nReflection mechanism: synthesizes memories into higher level inferences over time and',\n",
              "   'page_span': [15],\n",
              "   'document_metadata': {'url': \"/content/drive/MyDrive/GenAI_Expts/rag_datasets/LLM Powered Autonomous Agents _ Lil'Log.pdf\",\n",
              "    'title': \"LLM Powered Autonomous Agents _ Lil'Log.pdf\"}}},\n",
              " {'id': '51daf8d8-02f2-41a0-995c-a202f601d616/chunks/c3',\n",
              "  'distance': 0.6332048773765564,\n",
              "  'entity': {'content': 'attempts to achieve a goal by observing the world and acting upon it using the tools that it \\nhas at its disposal. Agents are autonomous and can act independently of human intervention, \\nespecially when provided with proper goals or objectives they are meant to achieve. Agents \\ncan also be proactive in their approach to reaching their goals. Even in the absence of \\nexplicit instruction sets from a human, an agent can reason about what it should do next to \\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this \\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of \\nbuilding at the time of publication.\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational \\ncomponents that drive the agent’s behavior, actions, and decision making. The combination \\nof these components can be described as a cognitive architecture, and there are many',\n",
              "   'page_span': [5],\n",
              "   'document_metadata': {'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Newwhitepaper_Agents2.pdf',\n",
              "    'title': 'Newwhitepaper_Agents2.pdf'}}}]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_relevant_docs(\"pdf_rag\",query)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "PFkX9oWBLVq8"
      },
      "outputs": [],
      "source": [
        "def format_sources(relevant_docs: list):\n",
        "  \"\"\"Format relevant docs into consumable format by llm\"\"\"\n",
        "  sources = \"\"\n",
        "  for i, doc in enumerate(relevant_docs[0]):\n",
        "    sources += f\"<source{i + 1}> {doc['entity']['content']} </source{i + 1}>\"\n",
        "\n",
        "  return sources\n",
        "\n",
        "def get_citation_urls(relevant_docs: list):\n",
        "  \"\"\"Format relevant docs to urls\"\"\"\n",
        "  citations = []\n",
        "  for doc in relevant_docs[0]:\n",
        "    citation_content = {\n",
        "        \"title\": doc['entity']['document_metadata']['title'],\n",
        "        \"url\": doc['entity']['document_metadata']['url'] + \"#page=\" + str(min(doc['entity']['page_span']))\n",
        "    }\n",
        "    citations.append(citation_content)\n",
        "\n",
        "  return citations\n",
        "def format_citations(citations: list):\n",
        "  \"\"\"Format citations to display\"\"\"\n",
        "  citation_str = \"\"\n",
        "  for citation in citations:\n",
        "    citation_str += f\"[{citation['title']}]({citation['url']})<br>\"\n",
        "\n",
        "  return citation_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "aU6jArLrMtj5",
        "outputId": "a2af71cd-fef0-4d53-c008-d93730714888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.004326s.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<source1> 1. Chilli con Carne \\n  \\n \\n \\nThis  was  one  of  the  first  easy \\nrecipes I learnt to cook and can be \\nmastered in just a few attempts! It’s \\nhealthy, cheap and tasty and makes \\nthe ultimate student food. You can \\nmake a large batch of this and then \\nstore in the fridge for days or freeze \\nfor weeks. \\n \\n \\nIngredients \\nGround/minced beef 500g \\n1 Large onion chopped \\n2-\\xad‐3 Cloves of Garlic \\n1-\\xad‐2 Tins of chopped tomatoes 400g \\nSqueeze of tomato puree \\n1 teaspoon of chilli powder (or to taste) \\n1 teaspoon of ground cumin \\ndash of Worcester sauce \\nSprinkle of salt and pepper \\n1 Chopped red pepper \\n1 tin of drained kidney beans 400g \\n \\nMethod \\n1) Fry the onion in a hot pan with oil until nearly brown then add chopped \\ngarlic \\n2) Add the mince and stir until brown -\\xad‐ drain any excess fat if desired \\n3) Add all dried spices and seasoning then reduce heat and add chopped \\ntomatoes \\n4) Stir well and add tomato puree and worcester sauce then leave to simmer </source1><source2> Quick and easy recipes \\n \\nStart cooking today! \\nYou can do it! \\n \\nCooking meals is much cheaper than eating outside. This is true everywhere \\nbut even more in Geneva... Take turns and cook for your roommate and your \\nfriends! You’ll save even more money – and the kitchen will probably stay \\ncleaner... \\n \\nI have selected only a few recipes from these websites: \\nhttp://www.studentrecipes.com/ \\nhttp://www.squidoo.com/studentfood#module55467342 \\n \\n \\nBon appétit! \\n \\nINDEX \\n1. Chili con carne \\n2. Beef in beer \\n3. Pad Thai Chicken \\n4. Thai Green Curry \\n5. Pineapple chicken \\n6. Vegetarian rice \\n7. Omelette \\n8. Egg fried rice \\n9. Salmon in the oven \\n10. \\nBasic pasta \\n11. \\nCarbonara al funghi \\n12. \\nSimple spaghetti bolognaise \\n13. \\nCucumber salad \\n14. \\nGoat cheese and Beetroot salad \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n1. Chilli con Carne \\n  \\n \\n \\nThis  was  one  of  the  first  easy \\nrecipes I learnt to cook and can be \\nmastered in just a few attempts! It’s \\nhealthy, cheap and tasty and makes </source2><source3> 3) Add all dried spices and seasoning then reduce heat and add chopped \\ntomatoes \\n4) Stir well and add tomato puree and worcester sauce then leave to simmer \\nfor about an hour (less if you’re in a rush) \\n5) Add the chopped red pepper and continue to simmer for 5 mins, then add \\nthe tin of drained kidney beans and cook for a further 5 mins. If the chilli \\nbecome to dry at any point just add a bit of water. \\nServe with rice, jacket potatoes or pasta! \\n2. Beef in Beer \\n \\nIngredients \\n500g of cheap beef pieces (stewing steak usually is cheap) \\n500 mls of real ale \\n4 large onions \\n2 fat cloves of garlic \\ntbsp of plain flour \\nMethod \\nRaw beef, roll it in the flour...fry 4-\\xad‐5 pieces at a time in a hot saucepan to \\nseal...put to one side when all done \\n \\ncut the onions into quarters...fry in the same frying pan as the beef...to soak up \\nthe juices... \\n \\nTowards the end of frying the onion (they should be nicely brown round the \\nedges) throw in the smashed up cloves of garlic. </source3><source4> the juices... \\n \\nTowards the end of frying the onion (they should be nicely brown round the \\nedges) throw in the smashed up cloves of garlic. \\n \\nTransfer the beef, onions, garlic to a deep casserole dish. Stir in the remaining \\nflour \\n \\npour on the bottle of beer -\\xad‐ really, it isn't going to be wasted!!! \\n \\nCover, and cook at 150 for about 2 and a half hours...will be gorgeous when it \\ncomes out, trust me...serve with mash \\n \\n3. Pad Thai Chicken \\n4 servings \\nIngredients \\nEssential items: \\nRice noodles (can use another type of noodle but read packet instructions for \\nhow to cook) \\n2 chicken breasts \\nsalt and pepper \\n3 medium red chillis (chopped) \\n3 spring onions (chopped) \\n2 eggs \\nCorriander (if I don't have any corriander i substitute for some dried mint) \\n1 tbsp lemon juice \\n1 tbsp brown sugar \\n2 cloves of garlic (minced) \\n3 tbsp of fish sauce \\n \\nThe following ingredients aren't essential but if you can afford them they taste \\nfantastic in this dish \\n60g cooked shrimp </source4><source5> packaging guidelines. \\n12. Simple Spaghetti Bolognaise  \\nA Classic College Recipe -\\xad‐ Simplified \\nI have tried to make this dish as simple as \\npossible, as you get better you can add more ingredients -\\xad‐ try cooking on a \\nlower heat for longer if you have the time. \\n \\nIngredients \\n500g beef mince \\n1 tbsp olive oil \\n1 onion, finely chopped \\n1 cup beef stock \\nclove of garlic \\nCarrots, mushroom and celery (optional) \\n1 tsp dried mixed or Italian-\\xad‐style herbs \\nSquirt of tomato paste \\n1 x 425g passatta or chopped tomatoes \\n \\nMethod \\nHeat oil in a heavy-\\xad‐based pan and add onion, stirring over a moderate heat for \\n1-\\xad‐2 minutes. \\n \\nAdd mince, stirring constantly until well browned. \\n \\nStir in remaining ingredients. \\n \\nCover and simmer gently for about 30 minutes, or until cooked through (if \\nusing chopped tomatoes this will take a bit longer. \\n \\nServe with your favourite spaghetti, bread and salad. \\n \\n13. Cucumber salad \\nIngredients \\n1 cucumber, sliced \\n3 tbsp sugar </source5>\""
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "format_sources(get_relevant_docs(\"pdfrag\",query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIq2tXzrMx_e",
        "outputId": "139a27de-24f5-4b74-a17b-78e7167e6e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.003946s.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'title': 'Easy_recipes.pdf',\n",
              "  'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=2'},\n",
              " {'title': 'Easy_recipes.pdf',\n",
              "  'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=1'},\n",
              " {'title': 'Easy_recipes.pdf',\n",
              "  'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=3'},\n",
              " {'title': 'Easy_recipes.pdf',\n",
              "  'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=3'},\n",
              " {'title': 'Easy_recipes.pdf',\n",
              "  'url': '/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=9'}]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_citation_urls(get_relevant_docs(\"pdfrag\",query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "4z4_jjj3SY_Z",
        "outputId": "57c0bd1f-e1bc-474e-9305-3f07d07742c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.004054s.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=2) <br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=1) <br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=3) <br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=3) <br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=9) <br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(format_citations(get_citation_urls(get_relevant_docs(\"pdfrag\",query)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVT-n2Kg7bT"
      },
      "source": [
        "#### Intialize LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqRI6mKzQFLk"
      },
      "outputs": [],
      "source": [
        "LLM_CONFIGS = {\n",
        "    \"gemini-1.5-flash\": {\n",
        "        \"model\": \"gemini-1.5-flash-002\",\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"temperature\": 0.0,\n",
        "    },\n",
        "    \"gemini-1.5-pro\": {\n",
        "        \"model\": \"gemini-1.5-pro-002\",\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"temperature\": 1.0,\n",
        "    },\n",
        "    \"gemini-2.0-flash\": {\n",
        "        \"model\": \"gemini-2.0-flash-001\",\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"temperature\": 0.0,\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhBZAkm1vib_"
      },
      "outputs": [],
      "source": [
        "chat_model = ChatGoogleGenerativeAI(**LLM_CONFIGS[\"gemini-2.0-flash\"],google_api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJE6twGwg9-H"
      },
      "source": [
        "#### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67brmURDRJx8"
      },
      "outputs": [],
      "source": [
        "# This can be used if user asks unrelated questions, but current models are intelligent enough to do this by default\n",
        "FALLBACK_NO_MATCH = \"I'm sorry, I'm unable to answer this question based on the data I have access to. Please try rephrasing your question or be more specific.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fyfTYcvKoEk"
      },
      "outputs": [],
      "source": [
        "# Prompt for generation of response using query and retrieved sources\n",
        "rag_template = \"\"\"You are a friendly and helpful chat bot. Your role is to help users by answering questions based on the sources provided.\n",
        "Maintain a friendly and supportive tone while providing accurate and helpful responses.\n",
        "Avoid adding any other explanation.\n",
        "Sources: {sources}\n",
        "\n",
        "User Query: {user_query}\n",
        "Response:\n",
        "\"\"\"\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "rag_chain = rag_prompt | chat_model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6c_4R_MvP5A"
      },
      "outputs": [],
      "source": [
        "# Prompt for generation of response using query and retrieved sources (multiturn)\n",
        "rag_template_multiturn = \"\"\"You are a friendly and helpful chat bot. Your role is to help users by answering questions based on the sources provided.\n",
        "Maintain a friendly and supportive tone while providing accurate and helpful responses. Utilize the provided conversation history to understand previous interactions.\n",
        "Avoid adding any other explanation.\n",
        "Sources: {sources}\n",
        "\n",
        "Conversation History: {chat_history}\n",
        "\n",
        "User Query: {user_query}\n",
        "Response:\n",
        "\"\"\"\n",
        "rag_prompt_multiturn = ChatPromptTemplate.from_messages([\n",
        "       (\"system\", rag_template_multiturn),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{user_query}\"),\n",
        "])\n",
        "rag_chain_multiturn = rag_prompt_multiturn | chat_model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-g7DD64vBy_"
      },
      "outputs": [],
      "source": [
        "# Prompt for rewriting query based on previous history\n",
        "query_rewrite_template = \"\"\"Given the following conversation history and user followup question, rephrase the user query to be a standalone question.\n",
        "The rephrased question should be understood without the chat history and should be meaningful and complete on its own.\n",
        "If the query is independent and is not related to the chat history, then return it as is. In any other case, rewrite the query.\n",
        "\n",
        "Conversation History: {chat_history}\n",
        "User Query: {question}\n",
        "\n",
        "Standalone Question:\n",
        "\"\"\"\n",
        "query_rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "       (\"system\", query_rewrite_template),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{question}\"),\n",
        "])\n",
        "rewrite_chain = query_rewrite_prompt | chat_model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEj3B0mHhAp0"
      },
      "source": [
        "#### Generation (Single turn)\n",
        "This a stateless interaction, where no conversation history is taken into account. Single turn results can be provided using the below logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "HkQAK6f5wdeG"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query: str):\n",
        "  \"\"\"Generate answer to the query from vector database\"\"\"\n",
        "  relevant_docs = get_relevant_docs(collection_name=\"pdfrag\",\n",
        "                           query=query)\n",
        "\n",
        "  sources = format_sources(relevant_docs)\n",
        "  citations = get_citation_urls(relevant_docs)\n",
        "  response = rag_chain.invoke({\n",
        "      \"sources\": sources,\n",
        "      \"user_query\": query\n",
        "  })\n",
        "\n",
        "  return response, citations\n",
        "  # For streaming\n",
        "  # response_text=\"\"\n",
        "  # for token in response:\n",
        "  #   response_text+=token\n",
        "  #   print(token, end=\"\", flush=True)\n",
        "\n",
        "  # return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "Ni7VysEv2zCV",
        "outputId": "ce013291-8b7a-4845-bf5c-10cbc3e342cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.011637s.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Here's how to make Chilli con Carne:\n",
              "\n",
              "**Ingredients**\n",
              "*   Ground/minced beef 500g\n",
              "*   1 Large onion chopped\n",
              "*   2-3 Cloves of Garlic\n",
              "*   1-2 Tins of chopped tomatoes 400g\n",
              "*   Squeeze of tomato puree\n",
              "*   1 teaspoon of chilli powder (or to taste)\n",
              "*   1 teaspoon of ground cumin\n",
              "*   dash of Worcester sauce\n",
              "*   Sprinkle of salt and pepper\n",
              "*   1 Chopped red pepper\n",
              "*   1 tin of drained kidney beans 400g\n",
              "\n",
              "**Method**\n",
              "\n",
              "1.  Fry the onion in a hot pan with oil until nearly brown then add chopped garlic\n",
              "2.  Add the mince and stir until brown - drain any excess fat if desired\n",
              "3.  Add all dried spices and seasoning then reduce heat and add chopped tomatoes\n",
              "4.  Stir well and add tomato puree and worcester sauce then leave to simmer for about an hour (less if you’re in a rush)\n",
              "5.  Add the chopped red pepper and continue to simmer for 5 mins, then add the tin of drained kidney beans and cook for a further 5 mins. If the chilli become to dry at any point just add a bit of water.\n",
              "\n",
              "Serve with rice, jacket potatoes or pasta!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=2)<br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=1)<br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=3)<br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=3)<br>[Easy_recipes.pdf](/content/drive/MyDrive/GenAI_Expts/rag_datasets/Easy_recipes.pdf#page=9)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"How to make chilli con carne?\"\n",
        "response, citations = generate_answer(query)\n",
        "display(Markdown(response))\n",
        "display(Markdown(format_citations(citations)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWEpRP_jN3wl"
      },
      "source": [
        "#### Generation (Multiturn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWeHylDaINnG"
      },
      "outputs": [],
      "source": [
        "def generate_answer_multiturn(query: str, chat_history: list):\n",
        "  \"\"\"Generate answer to the query from vector database\"\"\"\n",
        "  relevant_docs = get_relevant_docs(collection_name=\"pdfrag\",\n",
        "                           query=query)\n",
        "\n",
        "  sources = format_sources(relevant_docs)\n",
        "\n",
        "  response = rag_chain_multiturn.invoke({\n",
        "      \"sources\": sources,\n",
        "      \"user_query\": query,\n",
        "      \"chat_history\": chat_history\n",
        "  })\n",
        "  # Since an output parser is used with the chain, response is added as AI message, else can be added directly\n",
        "  chat_history.append(HumanMessage(content=query))\n",
        "  chat_history.append(AIMessage(content=response))\n",
        "\n",
        "  return response, chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "VQhaid97IOfx",
        "outputId": "e0590ac0-0b9f-483a-d101-287428dd6f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.004217s.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Chilli con Carne is easy to master and is healthy, cheap and tasty! Here is what you need:\n",
              "\n",
              "Ingredients:\n",
              "- Ground/minced beef 500g\n",
              "- 1 Large onion chopped\n",
              "- 2-3 Cloves of Garlic\n",
              "- 1-2 Tins of chopped tomatoes 400g\n",
              "- Squeeze of tomato puree\n",
              "- 1 teaspoon of chilli powder (or to taste)\n",
              "- 1 teaspoon of ground cumin\n",
              "- dash of Worcester sauce\n",
              "- Sprinkle of salt and pepper\n",
              "- 1 Chopped red pepper\n",
              "- 1 tin of drained kidney beans 400g\n",
              "\n",
              "Method:\n",
              "1) Fry the onion in a hot pan with oil until nearly brown then add chopped garlic\n",
              "2) Add the mince and stir until brown - drain any excess fat if desired\n",
              "3) Add all dried spices and seasoning then reduce heat and add chopped tomatoes\n",
              "4) Stir well and add tomato puree and worcester sauce then leave to simmer for about an hour (less if you’re in a rush)\n",
              "5) Add the chopped red pepper and continue to simmer for 5 mins, then add the tin of drained kidney beans and cook for a further 5 mins. If the chilli become to dry at any point just add a bit of water.\n",
              "Serve with rice, jacket potatoes or pasta!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"How to make chilli con carne?\"\n",
        "response, history = generate_answer_multiturn(query, [])\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "a2MHV23FIOcK",
        "outputId": "68d0c966-1173-4a41-c5c3-3a62031d1e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved in 0.004106s.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Certainly! Here are the ingredients for Chilli con Carne:\n",
              "\n",
              "- Ground/minced beef 500g\n",
              "- 1 Large onion chopped\n",
              "- 2-3 Cloves of Garlic\n",
              "- 1-2 Tins of chopped tomatoes 400g\n",
              "- Squeeze of tomato puree\n",
              "- 1 teaspoon of chilli powder (or to taste)\n",
              "- 1 teaspoon of ground cumin\n",
              "- dash of Worcester sauce\n",
              "- Sprinkle of salt and pepper\n",
              "- 1 Chopped red pepper\n",
              "- 1 tin of drained kidney beans 400g"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"What were the ingredients again?\"\n",
        "followup_response, history = generate_answer_multiturn(query, history)\n",
        "display(Markdown(followup_response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcVEhD89IOZs",
        "outputId": "fce43ddb-6da7-43d4-d011-7bfb2465a9a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='How to make chilli con carne?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Chilli con Carne is easy to master and is healthy, cheap and tasty! Here is what you need:\\n\\nIngredients:\\n- Ground/minced beef 500g\\n- 1 Large onion chopped\\n- 2-3 Cloves of Garlic\\n- 1-2 Tins of chopped tomatoes 400g\\n- Squeeze of tomato puree\\n- 1 teaspoon of chilli powder (or to taste)\\n- 1 teaspoon of ground cumin\\n- dash of Worcester sauce\\n- Sprinkle of salt and pepper\\n- 1 Chopped red pepper\\n- 1 tin of drained kidney beans 400g\\n\\nMethod:\\n1) Fry the onion in a hot pan with oil until nearly brown then add chopped garlic\\n2) Add the mince and stir until brown - drain any excess fat if desired\\n3) Add all dried spices and seasoning then reduce heat and add chopped tomatoes\\n4) Stir well and add tomato puree and worcester sauce then leave to simmer for about an hour (less if you’re in a rush)\\n5) Add the chopped red pepper and continue to simmer for 5 mins, then add the tin of drained kidney beans and cook for a further 5 mins. If the chilli become to dry at any point just add a bit of water.\\nServe with rice, jacket potatoes or pasta!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What were the ingredients again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Certainly! Here are the ingredients for Chilli con Carne:\\n\\n- Ground/minced beef 500g\\n- 1 Large onion chopped\\n- 2-3 Cloves of Garlic\\n- 1-2 Tins of chopped tomatoes 400g\\n- Squeeze of tomato puree\\n- 1 teaspoon of chilli powder (or to taste)\\n- 1 teaspoon of ground cumin\\n- dash of Worcester sauce\\n- Sprinkle of salt and pepper\\n- 1 Chopped red pepper\\n- 1 tin of drained kidney beans 400g', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mW9MwnF56b-0",
        "WsC0SbfTezgE",
        "ognLVOY5hNv6",
        "-sI6D1D-e14m",
        "C8C0jrH5fJPA",
        "WC4p_C7jPkpU",
        "-hSIWMP2Pm3n",
        "a422iax-Pqx4",
        "9aVT-n2Kg7bT",
        "PJE6twGwg9-H",
        "uWEpRP_jN3wl"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
